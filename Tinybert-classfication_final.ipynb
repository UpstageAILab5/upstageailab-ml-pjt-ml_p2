{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# venvì—ì„œ ì‹¤í–‰í•˜ì\n",
    "# pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers torch evaluate accelerate\n",
    "# transformer[] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"transformers[torch]\" accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./tinybert_model.csv', index_col=0)\n",
    "data.head()\n",
    "\n",
    "# sageMaker => AI + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"This is by far THE WORST movie i have ever watched. I've seen some pretty awful movies in my time but this ones takes the cake, no, wait, i mean the the whole damn bakery. It is so bad that i believe a word to describe the way you will feel after watching this atrocity has yet to be created. Please just do yourself a favor, if you ever get the urge to watch this and watch thirty minutes of that annoying purple dinosaur Barney, then multiply that thirty times fold and you would still only get a small fraction of the horror you would be in store for. In summation, i guess you really can call it a horror movie, but only if you're willing to be scared senseless by the worst acting in the business and utterly pointless story.<br /><br />Real Rating, -10 Disgusting\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "display(dataset)\n",
    "display(dataset['train'][0]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35000/35000 [00:03<00:00, 11217.25 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15000/15000 [00:01<00:00, 11652.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__', 'label'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label2id = {'negative':0, 'positive':1}\n",
    "dataset = dataset.map(lambda x: {'label':label2id[x['sentiment']]})\n",
    "display(dataset)\n",
    "display(dataset['train'][0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"This is by far THE WORST movie i have ever watched. I've seen some pretty awful movies in my time but this ones takes the cake, no, wait, i mean the the whole damn bakery. It is so bad that i believe a word to describe the way you will feel after watching this atrocity has yet to be created. Please just do yourself a favor, if you ever get the urge to watch this and watch thirty minutes of that annoying purple dinosaur Barney, then multiply that thirty times fold and you would still only get a small fraction of the horror you would be in store for. In summation, i guess you really can call it a horror movie, but only if you're willing to be scared senseless by the worst acting in the business and utterly pointless story.<br /><br />Real Rating, -10 Disgusting\",\n",
       " 'sentiment': 'negative',\n",
       " '__index_level_0__': 32452,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„°\n",
    "from transformers import AutoTokenizer # GPU\n",
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", use_fast=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 2026, 2034, 3653, 2094, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"This is my first pred\")\n",
    "# tokenizer(\"ì•ˆë…•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 2011, 2521, 1996, 5409, 3185, 1045, 2031, 2412, 3427, 1012, 1045, 1005, 2310, 2464, 2070, 3492, 9643, 5691, 1999, 2026, 2051, 2021, 2023, 3924, 3138, 1996, 9850, 1010, 2053, 1010, 3524, 1010, 1045, 2812, 1996, 1996, 2878, 4365, 18112, 1012, 2009, 2003, 2061, 2919, 2008, 1045, 2903, 1037, 2773, 2000, 6235, 1996, 2126, 2017, 2097, 2514, 2044, 3666, 2023, 2012, 21735, 2038, 2664, 2000, 2022, 2580, 1012, 3531, 2074, 2079, 4426, 1037, 5684, 1010, 2065, 2017, 2412, 2131, 1996, 9075, 2000, 3422, 2023, 1998, 3422, 4228, 2781, 1997, 2008, 15703, 6379, 15799, 15377, 1010, 2059, 4800, 22086, 2008, 4228, 2335, 10671, 1998, 2017, 2052, 2145, 2069, 2131, 1037, 2235, 12884, 1997, 1996, 5469, 2017, 2052, 2022, 1999, 3573, 2005, 1012, 1999, 7680, 28649, 1010, 1045, 3984, 2017, 2428, 2064, 2655, 2009, 1037, 5469, 3185, 1010, 2021, 2069, 2065, 2017, 1005, 2128, 5627, 2000, 2022, 6015, 3168, 3238, 2011, 1996, 5409, 3772, 1999, 1996, 2449, 1998, 12580, 23100, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2613, 5790, 1010, 1011, 2184, 19424, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['train'][0]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    temp = tokenizer(batch['review'], padding=True, truncation=True, max_length=300)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35000/35000 [00:49<00:00, 702.44 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15000/15000 [00:17<00:00, 871.68 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"This is by far THE WORST movie i have ever watched. I've seen some pretty awful movies in my time but this ones takes the cake, no, wait, i mean the the whole damn bakery. It is so bad that i believe a word to describe the way you will feel after watching this atrocity has yet to be created. Please just do yourself a favor, if you ever get the urge to watch this and watch thirty minutes of that annoying purple dinosaur Barney, then multiply that thirty times fold and you would still only get a small fraction of the horror you would be in store for. In summation, i guess you really can call it a horror movie, but only if you're willing to be scared senseless by the worst acting in the business and utterly pointless story.<br /><br />Real Rating, -10 Disgusting\",\n",
       " 'sentiment': 'negative',\n",
       " '__index_level_0__': 32452,\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  2023,\n",
       "  2003,\n",
       "  2011,\n",
       "  2521,\n",
       "  1996,\n",
       "  5409,\n",
       "  3185,\n",
       "  1045,\n",
       "  2031,\n",
       "  2412,\n",
       "  3427,\n",
       "  1012,\n",
       "  1045,\n",
       "  1005,\n",
       "  2310,\n",
       "  2464,\n",
       "  2070,\n",
       "  3492,\n",
       "  9643,\n",
       "  5691,\n",
       "  1999,\n",
       "  2026,\n",
       "  2051,\n",
       "  2021,\n",
       "  2023,\n",
       "  3924,\n",
       "  3138,\n",
       "  1996,\n",
       "  9850,\n",
       "  1010,\n",
       "  2053,\n",
       "  1010,\n",
       "  3524,\n",
       "  1010,\n",
       "  1045,\n",
       "  2812,\n",
       "  1996,\n",
       "  1996,\n",
       "  2878,\n",
       "  4365,\n",
       "  18112,\n",
       "  1012,\n",
       "  2009,\n",
       "  2003,\n",
       "  2061,\n",
       "  2919,\n",
       "  2008,\n",
       "  1045,\n",
       "  2903,\n",
       "  1037,\n",
       "  2773,\n",
       "  2000,\n",
       "  6235,\n",
       "  1996,\n",
       "  2126,\n",
       "  2017,\n",
       "  2097,\n",
       "  2514,\n",
       "  2044,\n",
       "  3666,\n",
       "  2023,\n",
       "  2012,\n",
       "  21735,\n",
       "  2038,\n",
       "  2664,\n",
       "  2000,\n",
       "  2022,\n",
       "  2580,\n",
       "  1012,\n",
       "  3531,\n",
       "  2074,\n",
       "  2079,\n",
       "  4426,\n",
       "  1037,\n",
       "  5684,\n",
       "  1010,\n",
       "  2065,\n",
       "  2017,\n",
       "  2412,\n",
       "  2131,\n",
       "  1996,\n",
       "  9075,\n",
       "  2000,\n",
       "  3422,\n",
       "  2023,\n",
       "  1998,\n",
       "  3422,\n",
       "  4228,\n",
       "  2781,\n",
       "  1997,\n",
       "  2008,\n",
       "  15703,\n",
       "  6379,\n",
       "  15799,\n",
       "  15377,\n",
       "  1010,\n",
       "  2059,\n",
       "  4800,\n",
       "  22086,\n",
       "  2008,\n",
       "  4228,\n",
       "  2335,\n",
       "  10671,\n",
       "  1998,\n",
       "  2017,\n",
       "  2052,\n",
       "  2145,\n",
       "  2069,\n",
       "  2131,\n",
       "  1037,\n",
       "  2235,\n",
       "  12884,\n",
       "  1997,\n",
       "  1996,\n",
       "  5469,\n",
       "  2017,\n",
       "  2052,\n",
       "  2022,\n",
       "  1999,\n",
       "  3573,\n",
       "  2005,\n",
       "  1012,\n",
       "  1999,\n",
       "  7680,\n",
       "  28649,\n",
       "  1010,\n",
       "  1045,\n",
       "  3984,\n",
       "  2017,\n",
       "  2428,\n",
       "  2064,\n",
       "  2655,\n",
       "  2009,\n",
       "  1037,\n",
       "  5469,\n",
       "  3185,\n",
       "  1010,\n",
       "  2021,\n",
       "  2069,\n",
       "  2065,\n",
       "  2017,\n",
       "  1005,\n",
       "  2128,\n",
       "  5627,\n",
       "  2000,\n",
       "  2022,\n",
       "  6015,\n",
       "  3168,\n",
       "  3238,\n",
       "  2011,\n",
       "  1996,\n",
       "  5409,\n",
       "  3772,\n",
       "  1999,\n",
       "  1996,\n",
       "  2449,\n",
       "  1998,\n",
       "  12580,\n",
       "  23100,\n",
       "  2466,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2613,\n",
       "  5790,\n",
       "  1010,\n",
       "  1011,\n",
       "  2184,\n",
       "  19424,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 312)\n",
      "      (token_type_embeddings): Embedding(2, 312)\n",
      "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-3): 4 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
      "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0:'negative', 1:'positive'}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"huawei-noah/TinyBERT_General_4L_312D\",\n",
    "    num_labels=2,\n",
    "    label2id = label2id, # ë ˆì´ë¸” ë¬¸ìì˜ã„¹ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë§µí•‘\n",
    "    id2label = id2label  # ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ IDë¥¼ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•´ì„œ ì•Œë ¤ì¤Œ.\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "# Hugging Face evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ accuracy(ì •í™•ë„) ë©”íŠ¸ë¦­ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "accuracy = evaluate.load('accuracy')\n",
    "# compute_metrics í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ë ˆì´ë¸”ì„ ë°›ì•„ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_predëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ë ˆì´ë¸”ì„ í¬í•¨í•˜ëŠ” íŠœí”Œì…ë‹ˆë‹¤.\n",
    "    predictions, labels = eval_pred  # predictions: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’, labels: ì‹¤ì œ ì •ë‹µ ë ˆì´ë¸”\n",
    "    \n",
    "    # predictionsëŠ” ëª¨ë¸ì´ ì¶œë ¥í•œ í™•ë¥ ê°’ ë°°ì—´ì´ë©°, ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "    # np.argmaxëŠ” ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ê°’ì„ ê°€ì§„ í´ë˜ìŠ¤ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    predictions = np.argmax(predictions, axis=1)  # ê° ìƒ˜í”Œë³„ë¡œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ ì„ íƒ\n",
    "    \n",
    "    # accuracy.computeëŠ” ì˜ˆì¸¡ê°’(predictions)ê³¼ ì‹¤ì œê°’(labels)ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    # predictions: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤, references=labels: ì‹¤ì œ ì •ë‹µ í´ë˜ìŠ¤\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSKang\\a_movie_new_start\\myenv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSKang\\AppData\\Local\\Temp\\ipykernel_20176\\327344426.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì • ì •ì˜\n",
    "# ë‚´ ì»´í“¨í„° ì‹¤í–‰ í™˜ê²½ì—ì„œ ëŒë¦¬ê¸° í˜ë“¤ì–´ì„œ ì¼ë¶€ ë³€ê²½\n",
    "# args = TrainingArguments(\n",
    "#     output_dir='train_dir',               # í•™ìŠµ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬\n",
    "#     overwrite_output_dir=True,            # ì¶œë ¥ ë””ë ‰í„°ë¦¬ì— ì´ë¯¸ ìˆëŠ” íŒŒì¼ì„ ë®ì–´ì“¸ì§€ ì—¬ë¶€\n",
    "#     num_train_epochs=3,                   # í•™ìŠµí•  ì—í¬í¬(epoch) ìˆ˜\n",
    "#     learning_rate=2e-5,                   # í•™ìŠµë¥  (learning rate)\n",
    "#     per_device_train_batch_size=32,       # ê° ë””ë°”ì´ìŠ¤(ì˜ˆ: GPU)ë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸°\n",
    "#     per_device_eval_batch_size=32,        # ê° ë””ë°”ì´ìŠ¤ë‹¹ í‰ê°€ ë°°ì¹˜ í¬ê¸°\n",
    "#     evaluation_strategy='epoch'           # í‰ê°€ ì „ëµ (ì—¬ê¸°ì„œëŠ” ë§¤ ì—í¬í¬ë§ˆë‹¤ í‰ê°€)\n",
    "# )\n",
    "args = TrainingArguments(\n",
    "    output_dir='train_dir',               # í•™ìŠµ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬\n",
    "    overwrite_output_dir=True,            # ì¶œë ¥ ë””ë ‰í„°ë¦¬ì— ì´ë¯¸ ìˆëŠ” íŒŒì¼ì„ ë®ì–´ì“¸ì§€ ì—¬ë¶€\n",
    "    num_train_epochs=1,                   # í•™ìŠµí•  ì—í¬í¬(epoch) ìˆ˜\n",
    "    learning_rate=5e-5,                   # í•™ìŠµë¥  (learning rate)\n",
    "    per_device_train_batch_size=8,       # ê° ë””ë°”ì´ìŠ¤(ì˜ˆ: GPU)ë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸°\n",
    "    per_device_eval_batch_size=8,        # ê° ë””ë°”ì´ìŠ¤ë‹¹ í‰ê°€ ë°°ì¹˜ í¬ê¸°\n",
    "    evaluation_strategy='steps',          # í‰ê°€ ì „ëµì„ 'steps'ë¡œ ë³€ê²½\n",
    "    save_steps=500,                       # ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ì£¼ê¸°ë¥¼ ëŠ˜ë¦¼\n",
    "    eval_steps=500                        # í‰ê°€ ì£¼ê¸°ë¥¼ ëŠ˜ë¦¼\n",
    "    # evaluation_strategy='epoch'           # í‰ê°€ ì „ëµ (ì—¬ê¸°ì„œëŠ” ë§¤ ì—í¬í¬ë§ˆë‹¤ í‰ê°€)\n",
    ")\n",
    "# Trainer ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ í•™ìŠµ ë° í‰ê°€ë¥¼ ê´€ë¦¬\n",
    "trainer = Trainer(\n",
    "    model=model,                          # í•™ìŠµí•  ëª¨ë¸\n",
    "    args=args,                            # í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "    train_dataset=dataset['train'],       # í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ì…‹\n",
    "    eval_dataset=dataset['test'],         # í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹\n",
    "    compute_metrics=compute_metrics,      # í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    tokenizer=tokenizer                   # í† í¬ë‚˜ì´ì € (í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚´ ì»´ì—ì„œ ëŒì•„ê°€ê¸° í˜ë“¤ì–´ì„œ ì¼ë¶€ ë³€ê²½ ë° ì£¼ì„ì²˜ë¦¬\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚´ ì»´ì—ì„œ ëŒì•„ê°€ê¸° í˜ë“¤ì–´ì„œ ì¼ë¶€ ë³€ê²½ ë° ì£¼ì„ì²˜ë¦¬\n",
    "# trainer.evaluate()\n",
    "# trainer.save_model('tinybert-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚´ ì»´ì—ì„œ ëŒì•„ê°€ê¸° í˜ë“¤ì–´ì„œ trainëœ í´ë”ë¥¼ ë³µì‚¬ í›„ add í•œ ì½”ë“œ\n",
    "tranined_model_path = 'tinybert-sentiment-analysis'  # ìˆ˜ì—…ì‹œê°„ ëª¨ë¸\n",
    "# tranined_model_path = './tinybert_model_test'      # íŒ€ì¥ë‹˜ trinined_model í´ë”\n",
    "tokenizer = AutoTokenizer.from_pretrained(tranined_model_path, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(tranined_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# classifier = pipeline('text-classification', model='tinybert-sentiment-analysis', device=device)\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "# classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ['This movie is soo boring, I waste my time','hi man','bye']\n",
    "data = ['ì¬ë¯¸ì—†ì–´, ì‹œê°„ë§Œ ë²„ë ¸ì–´','ì•ˆë…•','ì˜ê°€']\n",
    "#ë‚´ ì»´ì—ì„œ ëŒì•„ê°€ë„ë¡ ì£¼ì„ì²˜ë¦¬ í›„ ë‹¤ë¥¸ ì½”ë“œ add\n",
    "# classifier[data]\n",
    "\n",
    "\n",
    "#ë‚´ ì»´ì—ì„œ ëŒì•„ê°€ë„ë¡ Add\n",
    "# TextClassificationPipeline ê°ì²´ëŠ” ì¸ë±ì‹±ì„ ì§€ì›í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ì…ë‹ˆë‹¤. ëŒ€ì‹  classifier ê°ì²´ë¥¼ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤\n",
    "# ë¶„ë¥˜ ìˆ˜í–‰\n",
    "results = classifier(data)\n",
    "# print(results)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# í‰ê·  ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "average_score = sum(result['score'] for result in results) / len(results)\n",
    "\n",
    "# í‰ê·  ìŠ¤ì½”ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³„ì  ìƒì„± (5ì  ë§Œì )\n",
    "star_rating = round(average_score * 5)\n",
    "\n",
    "print(f\"Average Score: {average_score}\")\n",
    "print(f\"Star Rating: {star_rating} out of 5\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pkl ëª¨ë¸ ë¡œë“œ - ì£¼ì€ë‹˜ pkl\n",
    "# from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# # tranined_model_path = 'tinybert-sentiment-analysis'\n",
    "# tranined_model_path = './tinybert_model_test'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tranined_model_path, use_fast=True)\n",
    "\n",
    "# pkl_name = 'model_20241127_151546.pkl'\n",
    "\n",
    "# # CPUë¡œ ëª¨ë¸ ë¡œë“œ\n",
    "# loaded_model = torch.load(pkl_name, map_location=torch.device('cpu'))\n",
    "\n",
    "# # í…ìŠ¤íŠ¸ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ ì„¤ì •\n",
    "# classifier = pipeline('text-classification', model=loaded_model, tokenizer=tokenizer, device=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl ëª¨ë¸ ì €ì¥\n",
    "import pickle\n",
    "\n",
    "pkl_name = 'final_tinybert-sentiment-analysis.pkl'\n",
    "\n",
    "with open(pkl_name, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainëœ Tokenizer ì €ì¥\n",
    "tranined_model_path = 'tinybert-sentiment-analysis'  # ìˆ˜ì—…ì‹œê°„ ëª¨ë¸\n",
    "# tranined_model_path = 'tinybert_model_test'      # íŒ€ì¥ë‹˜ trinined_model í´ë”\n",
    "\n",
    "# tokenizer ì €ì¥ ê²½ë¡œ\n",
    "FInal_TOKENIZER_DIR = 'final_tokenizer'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tranined_model_path, use_fast=True)\n",
    "tokenizer.save_pretrained(FInal_TOKENIZER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë¡œë“œ\n",
    "with open(pkl_name, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ ì„¤ì •\n",
    "classifier = pipeline('text-classification', model=loaded_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(FInal_TOKENIZER_DIR, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ['This movie is soo boring, I waste my time','hi man','bye']\n",
    "data = ['ë„ˆë¬´ ì¬ë¯¸ìˆì–´, í€í€í€']\n",
    "#ë‚´ ì»´ì—ì„œ ëŒì•„ê°€ë„ë¡ ì£¼ì„ì²˜ë¦¬ í›„ ë‹¤ë¥¸ ì½”ë“œ add\n",
    "# classifier[data]\n",
    "\n",
    "\n",
    "#ë‚´ ì»´ì—ì„œ ëŒì•„ê°€ë„ë¡ Add\n",
    "# TextClassificationPipeline ê°ì²´ëŠ” ì¸ë±ì‹±ì„ ì§€ì›í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ì…ë‹ˆë‹¤. ëŒ€ì‹  classifier ê°ì²´ë¥¼ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤\n",
    "# ë¶„ë¥˜ ìˆ˜í–‰\n",
    "results = classifier(data)\n",
    "# print(results)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# ë³„ì  ê³„ì‚°\n",
    "total_score = 0\n",
    "for result in results:\n",
    "    if result['label'] == 'negative':\n",
    "        total_score += (1 - result['score']) * 5  # negativeì¼ ë•ŒëŠ” ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë‚®ì€ ë³„ì \n",
    "    elif result['label'] == 'positive':\n",
    "        total_score += result['score'] * 5  # positiveì¼ ë•ŒëŠ” ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë†’ì€ ë³„ì \n",
    "\n",
    "# í‰ê·  ë³„ì  ê³„ì‚°\n",
    "average_star_rating = round(total_score / len(results))\n",
    "\n",
    "print(f\"Average Star Rating: {average_star_rating} out of 5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
