{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# venv에서 실행하자\n",
    "# pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers torch evaluate accelerate\n",
    "# transformer[] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"transformers[torch]\" accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./tinybert_model.csv', index_col=0)\n",
    "data.head()\n",
    "\n",
    "# sageMaker => AI + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"This is by far THE WORST movie i have ever watched. I've seen some pretty awful movies in my time but this ones takes the cake, no, wait, i mean the the whole damn bakery. It is so bad that i believe a word to describe the way you will feel after watching this atrocity has yet to be created. Please just do yourself a favor, if you ever get the urge to watch this and watch thirty minutes of that annoying purple dinosaur Barney, then multiply that thirty times fold and you would still only get a small fraction of the horror you would be in store for. In summation, i guess you really can call it a horror movie, but only if you're willing to be scared senseless by the worst acting in the business and utterly pointless story.<br /><br />Real Rating, -10 Disgusting\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "display(dataset)\n",
    "display(dataset['train'][0]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:03<00:00, 11217.25 examples/s]\n",
      "Map: 100%|██████████| 15000/15000 [00:01<00:00, 11652.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__', 'label'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label2id = {'negative':0, 'positive':1}\n",
    "dataset = dataset.map(lambda x: {'label':label2id[x['sentiment']]})\n",
    "display(dataset)\n",
    "display(dataset['train'][0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"This is by far THE WORST movie i have ever watched. I've seen some pretty awful movies in my time but this ones takes the cake, no, wait, i mean the the whole damn bakery. It is so bad that i believe a word to describe the way you will feel after watching this atrocity has yet to be created. Please just do yourself a favor, if you ever get the urge to watch this and watch thirty minutes of that annoying purple dinosaur Barney, then multiply that thirty times fold and you would still only get a small fraction of the horror you would be in store for. In summation, i guess you really can call it a horror movie, but only if you're willing to be scared senseless by the worst acting in the business and utterly pointless story.<br /><br />Real Rating, -10 Disgusting\",\n",
       " 'sentiment': 'negative',\n",
       " '__index_level_0__': 32452,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터\n",
    "from transformers import AutoTokenizer # GPU\n",
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", use_fast=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 2026, 2034, 3653, 2094, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"This is my first pred\")\n",
    "# tokenizer(\"안녕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 2011, 2521, 1996, 5409, 3185, 1045, 2031, 2412, 3427, 1012, 1045, 1005, 2310, 2464, 2070, 3492, 9643, 5691, 1999, 2026, 2051, 2021, 2023, 3924, 3138, 1996, 9850, 1010, 2053, 1010, 3524, 1010, 1045, 2812, 1996, 1996, 2878, 4365, 18112, 1012, 2009, 2003, 2061, 2919, 2008, 1045, 2903, 1037, 2773, 2000, 6235, 1996, 2126, 2017, 2097, 2514, 2044, 3666, 2023, 2012, 21735, 2038, 2664, 2000, 2022, 2580, 1012, 3531, 2074, 2079, 4426, 1037, 5684, 1010, 2065, 2017, 2412, 2131, 1996, 9075, 2000, 3422, 2023, 1998, 3422, 4228, 2781, 1997, 2008, 15703, 6379, 15799, 15377, 1010, 2059, 4800, 22086, 2008, 4228, 2335, 10671, 1998, 2017, 2052, 2145, 2069, 2131, 1037, 2235, 12884, 1997, 1996, 5469, 2017, 2052, 2022, 1999, 3573, 2005, 1012, 1999, 7680, 28649, 1010, 1045, 3984, 2017, 2428, 2064, 2655, 2009, 1037, 5469, 3185, 1010, 2021, 2069, 2065, 2017, 1005, 2128, 5627, 2000, 2022, 6015, 3168, 3238, 2011, 1996, 5409, 3772, 1999, 1996, 2449, 1998, 12580, 23100, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2613, 5790, 1010, 1011, 2184, 19424, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['train'][0]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    temp = tokenizer(batch['review'], padding=True, truncation=True, max_length=300)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:49<00:00, 702.44 examples/s]\n",
      "Map: 100%|██████████| 15000/15000 [00:17<00:00, 871.68 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', '__index_level_0__', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"This is by far THE WORST movie i have ever watched. I've seen some pretty awful movies in my time but this ones takes the cake, no, wait, i mean the the whole damn bakery. It is so bad that i believe a word to describe the way you will feel after watching this atrocity has yet to be created. Please just do yourself a favor, if you ever get the urge to watch this and watch thirty minutes of that annoying purple dinosaur Barney, then multiply that thirty times fold and you would still only get a small fraction of the horror you would be in store for. In summation, i guess you really can call it a horror movie, but only if you're willing to be scared senseless by the worst acting in the business and utterly pointless story.<br /><br />Real Rating, -10 Disgusting\",\n",
       " 'sentiment': 'negative',\n",
       " '__index_level_0__': 32452,\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  2023,\n",
       "  2003,\n",
       "  2011,\n",
       "  2521,\n",
       "  1996,\n",
       "  5409,\n",
       "  3185,\n",
       "  1045,\n",
       "  2031,\n",
       "  2412,\n",
       "  3427,\n",
       "  1012,\n",
       "  1045,\n",
       "  1005,\n",
       "  2310,\n",
       "  2464,\n",
       "  2070,\n",
       "  3492,\n",
       "  9643,\n",
       "  5691,\n",
       "  1999,\n",
       "  2026,\n",
       "  2051,\n",
       "  2021,\n",
       "  2023,\n",
       "  3924,\n",
       "  3138,\n",
       "  1996,\n",
       "  9850,\n",
       "  1010,\n",
       "  2053,\n",
       "  1010,\n",
       "  3524,\n",
       "  1010,\n",
       "  1045,\n",
       "  2812,\n",
       "  1996,\n",
       "  1996,\n",
       "  2878,\n",
       "  4365,\n",
       "  18112,\n",
       "  1012,\n",
       "  2009,\n",
       "  2003,\n",
       "  2061,\n",
       "  2919,\n",
       "  2008,\n",
       "  1045,\n",
       "  2903,\n",
       "  1037,\n",
       "  2773,\n",
       "  2000,\n",
       "  6235,\n",
       "  1996,\n",
       "  2126,\n",
       "  2017,\n",
       "  2097,\n",
       "  2514,\n",
       "  2044,\n",
       "  3666,\n",
       "  2023,\n",
       "  2012,\n",
       "  21735,\n",
       "  2038,\n",
       "  2664,\n",
       "  2000,\n",
       "  2022,\n",
       "  2580,\n",
       "  1012,\n",
       "  3531,\n",
       "  2074,\n",
       "  2079,\n",
       "  4426,\n",
       "  1037,\n",
       "  5684,\n",
       "  1010,\n",
       "  2065,\n",
       "  2017,\n",
       "  2412,\n",
       "  2131,\n",
       "  1996,\n",
       "  9075,\n",
       "  2000,\n",
       "  3422,\n",
       "  2023,\n",
       "  1998,\n",
       "  3422,\n",
       "  4228,\n",
       "  2781,\n",
       "  1997,\n",
       "  2008,\n",
       "  15703,\n",
       "  6379,\n",
       "  15799,\n",
       "  15377,\n",
       "  1010,\n",
       "  2059,\n",
       "  4800,\n",
       "  22086,\n",
       "  2008,\n",
       "  4228,\n",
       "  2335,\n",
       "  10671,\n",
       "  1998,\n",
       "  2017,\n",
       "  2052,\n",
       "  2145,\n",
       "  2069,\n",
       "  2131,\n",
       "  1037,\n",
       "  2235,\n",
       "  12884,\n",
       "  1997,\n",
       "  1996,\n",
       "  5469,\n",
       "  2017,\n",
       "  2052,\n",
       "  2022,\n",
       "  1999,\n",
       "  3573,\n",
       "  2005,\n",
       "  1012,\n",
       "  1999,\n",
       "  7680,\n",
       "  28649,\n",
       "  1010,\n",
       "  1045,\n",
       "  3984,\n",
       "  2017,\n",
       "  2428,\n",
       "  2064,\n",
       "  2655,\n",
       "  2009,\n",
       "  1037,\n",
       "  5469,\n",
       "  3185,\n",
       "  1010,\n",
       "  2021,\n",
       "  2069,\n",
       "  2065,\n",
       "  2017,\n",
       "  1005,\n",
       "  2128,\n",
       "  5627,\n",
       "  2000,\n",
       "  2022,\n",
       "  6015,\n",
       "  3168,\n",
       "  3238,\n",
       "  2011,\n",
       "  1996,\n",
       "  5409,\n",
       "  3772,\n",
       "  1999,\n",
       "  1996,\n",
       "  2449,\n",
       "  1998,\n",
       "  12580,\n",
       "  23100,\n",
       "  2466,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2613,\n",
       "  5790,\n",
       "  1010,\n",
       "  1011,\n",
       "  2184,\n",
       "  19424,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 312)\n",
      "      (token_type_embeddings): Embedding(2, 312)\n",
      "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-3): 4 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
      "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0:'negative', 1:'positive'}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"huawei-noah/TinyBERT_General_4L_312D\",\n",
    "    num_labels=2,\n",
    "    label2id = label2id, # 레이블 문자영ㄹ 데이터를 숫자로 맵핑\n",
    "    id2label = id2label  # 예측한 클래스 ID를 사람이 이해할 수 있는 형태로 변환해서 알려줌.\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "# Hugging Face evaluate 라이브러리에서 accuracy(정확도) 메트릭을 불러옵니다.\n",
    "accuracy = evaluate.load('accuracy')\n",
    "# compute_metrics 함수는 모델의 예측값과 실제 레이블을 받아 정확도를 계산합니다.\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred는 모델의 예측값과 실제 레이블을 포함하는 튜플입니다.\n",
    "    predictions, labels = eval_pred  # predictions: 모델이 예측한 값, labels: 실제 정답 레이블\n",
    "    \n",
    "    # predictions는 모델이 출력한 확률값 배열이며, 가장 높은 확률을 가진 클래스를 선택합니다.\n",
    "    # np.argmax는 각 샘플에 대해 가장 높은 확률값을 가진 클래스의 인덱스를 반환합니다.\n",
    "    predictions = np.argmax(predictions, axis=1)  # 각 샘플별로 가장 높은 확률을 가진 클래스 선택\n",
    "    \n",
    "    # accuracy.compute는 예측값(predictions)과 실제값(labels)을 비교하여 정확도를 계산합니다.\n",
    "    # predictions: 모델이 예측한 클래스, references=labels: 실제 정답 클래스\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSKang\\a_movie_new_start\\myenv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSKang\\AppData\\Local\\Temp\\ipykernel_20176\\327344426.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# 모델 학습을 위한 하이퍼파라미터와 설정 정의\n",
    "# 내 컴퓨터 실행 환경에서 돌리기 힘들어서 일부 변경\n",
    "# args = TrainingArguments(\n",
    "#     output_dir='train_dir',               # 학습 결과를 저장할 디렉터리\n",
    "#     overwrite_output_dir=True,            # 출력 디렉터리에 이미 있는 파일을 덮어쓸지 여부\n",
    "#     num_train_epochs=3,                   # 학습할 에포크(epoch) 수\n",
    "#     learning_rate=2e-5,                   # 학습률 (learning rate)\n",
    "#     per_device_train_batch_size=32,       # 각 디바이스(예: GPU)당 학습 배치 크기\n",
    "#     per_device_eval_batch_size=32,        # 각 디바이스당 평가 배치 크기\n",
    "#     evaluation_strategy='epoch'           # 평가 전략 (여기서는 매 에포크마다 평가)\n",
    "# )\n",
    "args = TrainingArguments(\n",
    "    output_dir='train_dir',               # 학습 결과를 저장할 디렉터리\n",
    "    overwrite_output_dir=True,            # 출력 디렉터리에 이미 있는 파일을 덮어쓸지 여부\n",
    "    num_train_epochs=1,                   # 학습할 에포크(epoch) 수\n",
    "    learning_rate=5e-5,                   # 학습률 (learning rate)\n",
    "    per_device_train_batch_size=8,       # 각 디바이스(예: GPU)당 학습 배치 크기\n",
    "    per_device_eval_batch_size=8,        # 각 디바이스당 평가 배치 크기\n",
    "    evaluation_strategy='steps',          # 평가 전략을 'steps'로 변경\n",
    "    save_steps=500,                       # 모델을 저장하는 주기를 늘림\n",
    "    eval_steps=500                        # 평가 주기를 늘림\n",
    "    # evaluation_strategy='epoch'           # 평가 전략 (여기서는 매 에포크마다 평가)\n",
    ")\n",
    "# Trainer 객체를 생성하여 학습 및 평가를 관리\n",
    "trainer = Trainer(\n",
    "    model=model,                          # 학습할 모델\n",
    "    args=args,                            # 학습 파라미터 설정\n",
    "    train_dataset=dataset['train'],       # 학습에 사용할 데이터셋\n",
    "    eval_dataset=dataset['test'],         # 평가에 사용할 데이터셋\n",
    "    compute_metrics=compute_metrics,      # 평가 지표를 계산하는 함수\n",
    "    tokenizer=tokenizer                   # 토크나이저 (텍스트를 토큰으로 변환하는 도구)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내 컴에서 돌아가기 힘들어서 일부 변경 및 주석처리\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내 컴에서 돌아가기 힘들어서 일부 변경 및 주석처리\n",
    "# trainer.evaluate()\n",
    "# trainer.save_model('tinybert-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내 컴에서 돌아가기 힘들어서 train된 폴더를 복사 후 add 한 코드\n",
    "tranined_model_path = 'tinybert-sentiment-analysis'  # 수업시간 모델\n",
    "# tranined_model_path = './tinybert_model_test'      # 팀장님 trinined_model 폴더\n",
    "tokenizer = AutoTokenizer.from_pretrained(tranined_model_path, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(tranined_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# classifier = pipeline('text-classification', model='tinybert-sentiment-analysis', device=device)\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "# classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ['This movie is soo boring, I waste my time','hi man','bye']\n",
    "data = ['재미없어, 시간만 버렸어','안녕','잘가']\n",
    "#내 컴에서 돌아가도록 주석처리 후 다른 코드 add\n",
    "# classifier[data]\n",
    "\n",
    "\n",
    "#내 컴에서 돌아가도록 Add\n",
    "# TextClassificationPipeline 객체는 인덱싱을 지원하지 않기 때문에 발생하는 오류입니다. 대신 classifier 객체를 함수처럼 호출해야 합니다\n",
    "# 분류 수행\n",
    "results = classifier(data)\n",
    "# print(results)\n",
    "\n",
    "# 결과 출력\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# 평균 스코어 계산\n",
    "average_score = sum(result['score'] for result in results) / len(results)\n",
    "\n",
    "# 평균 스코어를 기반으로 별점 생성 (5점 만점)\n",
    "star_rating = round(average_score * 5)\n",
    "\n",
    "print(f\"Average Score: {average_score}\")\n",
    "print(f\"Star Rating: {star_rating} out of 5\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pkl 모델 로드 - 주은님 pkl\n",
    "# from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# # tranined_model_path = 'tinybert-sentiment-analysis'\n",
    "# tranined_model_path = './tinybert_model_test'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tranined_model_path, use_fast=True)\n",
    "\n",
    "# pkl_name = 'model_20241127_151546.pkl'\n",
    "\n",
    "# # CPU로 모델 로드\n",
    "# loaded_model = torch.load(pkl_name, map_location=torch.device('cpu'))\n",
    "\n",
    "# # 텍스트 분류 파이프라인 설정\n",
    "# classifier = pipeline('text-classification', model=loaded_model, tokenizer=tokenizer, device=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl 모델 저장\n",
    "import pickle\n",
    "\n",
    "pkl_name = 'final_tinybert-sentiment-analysis.pkl'\n",
    "\n",
    "with open(pkl_name, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train된 Tokenizer 저장\n",
    "tranined_model_path = 'tinybert-sentiment-analysis'  # 수업시간 모델\n",
    "# tranined_model_path = 'tinybert_model_test'      # 팀장님 trinined_model 폴더\n",
    "\n",
    "# tokenizer 저장 경로\n",
    "FInal_TOKENIZER_DIR = 'final_tokenizer'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tranined_model_path, use_fast=True)\n",
    "tokenizer.save_pretrained(FInal_TOKENIZER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "with open(pkl_name, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# 텍스트 분류 파이프라인 설정\n",
    "classifier = pipeline('text-classification', model=loaded_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# tokenizer 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(FInal_TOKENIZER_DIR, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ['This movie is soo boring, I waste my time','hi man','bye']\n",
    "data = ['너무 재미있어, 펀펀펀']\n",
    "#내 컴에서 돌아가도록 주석처리 후 다른 코드 add\n",
    "# classifier[data]\n",
    "\n",
    "\n",
    "#내 컴에서 돌아가도록 Add\n",
    "# TextClassificationPipeline 객체는 인덱싱을 지원하지 않기 때문에 발생하는 오류입니다. 대신 classifier 객체를 함수처럼 호출해야 합니다\n",
    "# 분류 수행\n",
    "results = classifier(data)\n",
    "# print(results)\n",
    "\n",
    "# 결과 출력\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# 별점 계산\n",
    "total_score = 0\n",
    "for result in results:\n",
    "    if result['label'] == 'negative':\n",
    "        total_score += (1 - result['score']) * 5  # negative일 때는 점수가 높을수록 낮은 별점\n",
    "    elif result['label'] == 'positive':\n",
    "        total_score += result['score'] * 5  # positive일 때는 점수가 높을수록 높은 별점\n",
    "\n",
    "# 평균 별점 계산\n",
    "average_star_rating = round(total_score / len(results))\n",
    "\n",
    "print(f\"Average Star Rating: {average_star_rating} out of 5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
