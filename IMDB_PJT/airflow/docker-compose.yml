version: "3.8"

services:
  airflow-webserver:
    build: .  # Dockerfile 위치
    container_name: tinybert_ml_airflow_webserver
    ports:
      - "8080:8080"  # Airflow Webserver accessible at localhost:8080
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5001  # Add MLflow connection
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////usr/local/airflow/airflow.db
      - DATA_PATH=/data/IMDB_Dataset.csv  # Dataset path
      - DATASET_PATH=/data/tk_dataset  # Processed dataset path
      - MODEL_PATH=/data/models/tinybert_model_test  # Model save path
    networks:
      - airflow_network
    volumes:
      - "./dags:/usr/local/airflow/dags"  # DAG files
      - "./models:/usr/local/airflow/models"  # Model storage
      - "./data:/data"  # Dataset and processed data storage
    depends_on:
      - mlflow
    command: >
      bash -c "airflow webserver --port 8080"

  airflow-scheduler:
    build: .  # Use the same Airflow image
    container_name: tinybert_ml_airflow_scheduler
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5001
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////usr/local/airflow/airflow.db
    networks:
      - airflow_network
    volumes:
      - "./dags:/usr/local/airflow/dags"
      - "./models:/usr/local/airflow/models"
      - "./data:/data"
    depends_on:
      - airflow-webserver
    command: >
      bash -c "airflow scheduler"

  mlflow:
    build:
      context: .  # Directory containing the Dockerfile for MLflow
      dockerfile: Dockerfile.mlflow
    container_name: tinybert_ml_mlflow
    ports:
      - "5001:5001"  # MLflow UI accessible at localhost:5001
    networks:
      - airflow_network
    volumes:
      - "./mlruns:/mlruns"  # Persist MLflow artifacts

networks:
  airflow_network:
    driver: bridge